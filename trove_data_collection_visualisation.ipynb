{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26265898-10ef-4b33-8ad2-5a5ffd670d6c",
   "metadata": {},
   "source": [
    "# Exploring Trove using the Trove API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774104ec-0a74-4989-8f36-61636320c539",
   "metadata": {},
   "source": [
    "### Setting Up\n",
    "####  Importing the required packages to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e67399-4ffe-4e81-8a53-1955814cc023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.express as px\n",
    "from typing import List, Tuple, Union, Any\n",
    "from pandas import DataFrame \n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f038db7-01b2-49b7-af88-3a615814966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89363c-2199-4281-b726-bf420d369a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eaa81e-ba48-4888-8a55-4b9e4750a63a",
   "metadata": {},
   "source": [
    "#### Functions written to access and visualise the data in the Trove API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9e58e-d808-4e53-a512-3b68cc31749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TroveAPIError(Exception):\n",
    "    \"\"\"Custom exception for errors when querying the Trove API.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_records(inc_ratelimit: bool = False, **kwargs: Any) -> Union[List[dict], Tuple[List[dict], List[int]]]:\n",
    "    \"\"\"Perform a single Trove API request search;\n",
    "        return a JSON result and optional rate limit information\"\"\"\n",
    "    \n",
    "    response = requests.get(**kwargs)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise TroveAPIError(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    try:\n",
    "        json_response = response.json()\n",
    "        result = json_response['category'][0]['records']\n",
    "    except (KeyError, IndexError, TypeError, ValueError) as e:\n",
    "        raise TroveAPIError(f\"Unexpected JSON response: {response.text}\") from e\n",
    "        \n",
    "    if inc_ratelimit:\n",
    "            return (\n",
    "                result, \n",
    "                [\n",
    "                    int(response.headers['X-RateLimit-Remaining-Minute']), \n",
    "                    int(response.headers['RateLimit-Reset'])\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_records_data(url:str, params:dict, api_key:str, all_pages:bool=True) -> List[dict]:\n",
    "    \"\"\"Search Trove using the API, return search results\"\"\"\n",
    "    data = []\n",
    "    seen = set()\n",
    "\n",
    "    try:\n",
    "        result = get_records(url=url, headers={\"X-API-KEY\": API_KEY}, params=params)\n",
    "        for record in result['article']:\n",
    "            record_key = record['id']\n",
    "            if record_key not in seen:\n",
    "                seen.add(record_key)\n",
    "                data.append(record)\n",
    "            else:\n",
    "                print(f\"duplicate skipped: {record_key}\")\n",
    "                \n",
    "    except KeyError as e:\n",
    "        if result['n'] == int(0):\n",
    "            print('No records found with search criteria:')\n",
    "            for key, val in params.items():\n",
    "                if key in ['category', 'q'] or key.startswith('l-'):\n",
    "                    print(f\"- {key}: {val}\")\n",
    "            return\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    if all_pages:\n",
    "        pbar = tqdm(total = result['total'], initial=len(data))\n",
    "        while 'next' in result.keys():\n",
    "            url = result['next']\n",
    "            result, rate_limit_remaining = get_records(url=url, headers={\"X-API-KEY\": API_KEY}, inc_ratelimit=True)\n",
    "            block = []\n",
    "            if 'article' in result.keys():\n",
    "                for record in result['article']:\n",
    "                    record_key = record['id']\n",
    "                    if record_key not in seen:\n",
    "                        seen.add(record_key)\n",
    "                        block.append(record)\n",
    "                    else:\n",
    "                        print(f\"duplicate skipped: {record_key}\")\n",
    "            data.extend(block)\n",
    "            pbar.update(len(block))\n",
    "    \n",
    "            if rate_limit_remaining[0] == 0:\n",
    "                print(f\"Rate limit reached. Pausing for {rate_limit_remaining[1] + 1} seconds...\")\n",
    "                time.sleep(rate_limit_remaining[1] + 1)\n",
    "    \n",
    "        pbar.close()\n",
    "\n",
    "    print(f\"Total results fetched: {len(data)}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_record_year_summary(search_term:str,  start_year:Union[str, int]=0, end_year:Union[str, int]=0, url:str='https://api.trove.nla.gov.au/v3/result', api_key:str=API_KEY) -> dict:\n",
    "    \"\"\"Calculate how many articles per year are found with the search term,\n",
    "    and visualise as a bar chart\"\"\"\n",
    "    params = {\n",
    "        'category': 'newspaper',\n",
    "        'q': search_term, \n",
    "        'encoding': 'json',\n",
    "        'n': 1\n",
    "    }\n",
    "    if start_year == 0: \n",
    "        params['sortby'] = 'dateasc'\n",
    "        result = get_records(url=url, headers={\"X-API-KEY\": API_KEY}, params=params)\n",
    "        start_year = int(result['article'][0]['date'][:4])\n",
    "    else:\n",
    "        start_year = int(start_year)\n",
    "\n",
    "    if end_year == 0: \n",
    "        params['sortby'] = 'datedesc'\n",
    "        result = get_records(url=url, headers={\"X-API-KEY\": API_KEY}, params=params)\n",
    "        end_year = int(result['article'][0]['date'][:4])\n",
    "    else:\n",
    "        end_year = int(end_year)\n",
    "    \n",
    "    print(f\"Getting records from: {start_year} to {end_year}\")\n",
    "\n",
    "    counts = {}\n",
    "\n",
    "    for year in tqdm(range(start_year, end_year)):\n",
    "        str_year = str(year)\n",
    "        params['l-decade'] = str_year[:3]\n",
    "        params['l-year'] = str_year\n",
    "\n",
    "        rate_limit_remaining = 10\n",
    "        result, rate_limit_remaining = get_records(url=url, headers={\"X-API-KEY\": API_KEY}, params=params, inc_ratelimit=True)\n",
    "        counts[str_year] = result['total']\n",
    "        if rate_limit_remaining[0] == 0:\n",
    "            time.sleep(rate_limit_remaining[1] + 1)\n",
    "\n",
    "\n",
    "    fig = px.bar(x=counts.keys(), y=counts.values(), title=f\"Count of Articles Found on Trove Newspapers with search term '{search_term}', {start_year}-{end_year}\", labels={'x': 'Year', 'y': 'articles found'})\n",
    "    fig.show(\"notebook\")\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "def get_records_count(url:str, params:dict, api_key:str):\n",
    "    \"\"\"Search Trove using the API, return number of search results\"\"\"\n",
    "    data = []\n",
    "    seen = set()\n",
    "\n",
    "    try:\n",
    "        result = get_records(url=url, params=params, headers={\"X-API-KEY\": API_KEY})\n",
    "\n",
    "        if result['n'] == int(0):\n",
    "            print('No records found with search criteria:')\n",
    "        else:\n",
    "            print(f\"{result['total']:,} records found with search criteria:\")\n",
    "        for key, val in params.items():\n",
    "                if key in ['category', 'q'] or key.startswith('l-'):\n",
    "                    print(f\"- {key}: {val}\")\n",
    "                \n",
    "    except TroveAPIError as e:\n",
    "        print(f\"API Error: {e}\")\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "def explode_dictonaries(df:DataFrame, col:str)->DataFrame:\n",
    "    \"\"\"Where a dataframe column contains dictionaries, splitting these into seperate columns\"\"\"\n",
    "    \n",
    "    print(f\"Exploding column {col}\")\n",
    "    location = df.columns.get_loc(col)\n",
    "    new_df = pd.json_normalize(df[col])\n",
    "    \n",
    "    for column in new_df:\n",
    "        new_df.rename(columns={column: col + '_' + column}, inplace=True)\n",
    "\n",
    "    updated_df = pd.concat(\n",
    "        [\n",
    "        df.iloc[:, :location-1], \n",
    "        new_df,\n",
    "        df.iloc[:, location+1:]\n",
    "        ], axis=1\n",
    "    )\n",
    "    return updated_df\n",
    "\n",
    "\n",
    "def split_trove_title(df:DataFrame, title_col:str)->DataFrame:\n",
    "    \"\"\"Split the newspaper title column into name, location, and run\"\"\"\n",
    "\n",
    "    new_df = df\n",
    "    title_location = new_df.columns.get_loc(title_col)\n",
    "    new_df.insert(title_location, 'newspaper_run', '')\n",
    "    new_df.insert(title_location, 'newspaper_state', '')\n",
    "    new_df.insert(title_location, 'newspaper_city', '')\n",
    "    new_df.insert(title_location, 'newspaper_title', '')\n",
    "    \n",
    "    try:\n",
    "        if new_df.loc[new_df[title_col].str.count('\\(') != 1].shape[0] == 0:\n",
    "            new_df['newspaper_title'] = new_df[title_col].str.split('(').str[0].str.strip()\n",
    "            new_df[title_col] = new_df[title_col].str.split('(').str[1].str.strip()\n",
    "        elif new_df.loc[new_df[title_col].str.count('\\(') > 1].shape[0] > 0:\n",
    "            new_df['temp'] = new_df[title_col].str.split('(')\n",
    "            new_df['newspaper_title'] = new_df['temp'].apply(lambda x: '('.join(x[:-1]))\n",
    "            new_df[title_col] = new_df['temp'].apply(lambda x: x[-1])\n",
    "            new_df.drop(columns=['temp'], inplace=True)\n",
    "        else:\n",
    "            print('could not split on bracket')\n",
    "        new_df['newspaper_run'] = new_df[title_col].str.split(':').str[-1].str.strip(')').str.strip()\n",
    "        new_df[title_col] = new_df.apply(lambda row: row[title_col].replace(row['newspaper_run'], '').strip(' : )'), axis=1)\n",
    "        new_df['newspaper_state'] = new_df[title_col].apply(lambda x: x.split(',')[-1].strip().strip('.') if ',' in x else x.strip('.'))\n",
    "        new_df['newspaper_city'] = new_df.apply(lambda row: row[title_col].replace(row['newspaper_state'], '').strip().strip(','), axis=1)\n",
    "                \n",
    "        new_df = new_df.drop(columns=[title_col])\n",
    "\n",
    "        if 'title_id' in new_df.columns:\n",
    "            new_df.rename(columns={'title_id': 'newspaper_id'}, inplace=True)\n",
    "        \n",
    "    except:\n",
    "        print('could not divide title column')\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fb29c-09b6-497a-b41f-a01056a2f2d1",
   "metadata": {},
   "source": [
    "### Search Terminology\n",
    "You can make your search query as simple or as complex as you need. It can include:\n",
    "- *single word* e.g. `'nationalism'` - will search for articles containing that word\n",
    "- *multiple words* e.g. `'nationalism federalism'` - will search for articles containing either of those words\n",
    "- *phrase* e.g. `'\"United States\"'` - will search for articles containing those words in that order - must be in double quotation marks\n",
    "- *BOOLEAN* terms (AND, OR, NOT) e.g. `'nationalism AND federalism'`\n",
    "\n",
    "Trove also has Indicies available that you can include in your search. Below are the ones available for newspapers:\n",
    "- *publictag*: public tags added by users. Exact match only.\n",
    "    - formatted as 'publictag:(sydney)' e.g. `'highway OR freeway publictag:(sydney)'`\n",
    "- *lastupdated*: Find everything that has been added or updated in Trove in a particular time frame. (Updates include text correction but not the addition of tags). Only dates in or after April 2012 are supported.\n",
    "    - formatted as 'lastupdated:[yyyy-mm-ddT00:00:00Z TO *]' or 'lastupdated:[yyyy-mm-ddT00:00:00Z TO yyyy-mm-ddT00:00:00Z]' e.g. `'nationalism lastupdated:[2012-01-01T00:00:00Z TO *]'`\n",
    "- *taglastupdated*: Find everything that has been tagged in Trove in a particular time frame. Only includes tags added, not tags deleted. Only dates in or after March 2012 are supported.\n",
    "    - formatted as 'taglastupdated:[yyyy-mm-ddT00:00:00Z TO *]' or 'taglastupdated:[yyyy-mm-ddT00:00:00Z TO yyyy-mm-ddT00:00:00Z]' e.g. `'nationalism taglastupdated:[2012-01-01T00:00:00Z TO *]'`\n",
    "- *commentlastupdated*: Find everything that has been commented on in Trove in a particular time frame. Only includes comments added, not comments deleted. Only dates in or after March 2012 are supported.\n",
    "    - formatted as 'commentlastupdated:[yyyy-mm-ddT00:00:00Z TO *]' or 'commentlastupdated:[yyyy-mm-ddT00:00:00Z TO yyyy-mm-ddT00:00:00Z]' e.g. `'nationalism commentlastupdated:[2017-01-01T00:00:00Z TO *]'`\n",
    "- *date*: date of publication\n",
    "    - formatted as 'date:[yyyy-mm-dd TO yyyy-mm-dd]' openended possible eg  'date:[yyyy-mm-dd TO *]', can just use year eg 'date:[* TO YYYY]' e.g. `'nationalism date:[1854 TO 1900]'`\n",
    "- *has*: Does this item have any tags or comments?\n",
    "    - formatted as 'has:tags' or 'has:comments' e.g. `'nationalism has:comments'`\n",
    "- *fulltext*: find items containing an exact word, not the stem\n",
    "    - eg 'fulltext:Jackes' returns items containing the exact word “Jackes” (not “Jack” or “Jacke”) e.g. `'fulltext:nationalism'`\n",
    "- *headingsAuthorAbstract*: find newspaper articles matching your search query to within the Title and first four lines\n",
    "    - formatted as 'headingsAuthorAbstract:y' e.g. `'fulltext:nationalism' headingsAuthorAbstract:y`\n",
    " \n",
    "*Can combine indicies, seperated by a space, e.g.*`\"q\": \"Art has:comments commentlastupdated:[2012-03-07T00:00:00Z TO *]\"` *searches for articles related to search term Art that have comments, and those comments were updated from the 7th March 2012 or later*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6087f36-bdc7-4096-b1b0-16384ac214aa",
   "metadata": {},
   "source": [
    "### Data Overviews\n",
    "using the `get_record_year_summary()` function created above, we can create a bar chart to see how many articles per year contain our search query. \n",
    "\n",
    "We need to include our search query, in single quotation marks. (if you have a phrase, the double quotation marks go inside the single quotation marks). \n",
    "\n",
    "There are some optional arguments that we can also include:\n",
    "- *start_year*: when we want the search to start from (default is to begin with the earliest record found\n",
    "- *end_year*: when we want the search to go to (default is to end with the latest record found\n",
    "- *api_key*: a Trove API key. (default is to use the variable `API_KEY` created above. You can run without an API Key, but it is much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00bf13-f429-44d4-bb64-6611565aa991",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_aus_overview = get_record_year_summary('\"white australia\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be619457-af4e-4b0b-a117-2d40e0fc8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# white_aus_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd569afb-ecaf-4111-8f3e-3337c5d91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_aus_overview = get_record_year_summary('\"white australia\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a25dfb-7789-420e-9050-2327c2efc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationalism = get_record_year_summary('\"nationalism\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd6e343-8daf-4073-8b42-3d99d1a43398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84521def-7d21-4220-9b1f-4170ebc45bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87ba789d-3578-4e99-8d2f-44f93c4f3ba5",
   "metadata": {},
   "source": [
    "### Collecting Metadata\n",
    "While we are not allowed to access the full text of newspaper articles in Trove, there is still some interesting things we can do with the metadata the API can provide. We can either run an API search using the Python code, or load a CSV file (i.e. one we have downloaded from the Trove interface, or have been provided from someone else. \n",
    "\n",
    "#### Searching with the API\n",
    "When we use the 'requests' library, we can construct the url to have all the parameters we need, and then get the data back in either XML or JSON format (for this notebook, we will be using JSON). \n",
    "\n",
    "We need to start with the beginning of the URL, which we are assigning to the variable `url`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1daa43a-5cc7-4741-8e2b-63ef30d3e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.trove.nla.gov.au/v3/result'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f7a2d-1877-483b-8950-e37a0984e38a",
   "metadata": {},
   "source": [
    "Then we need to specify our search parameters, which we are assigning to the variable `params`. \n",
    "\n",
    "Our search query goes with the key `q`. We can also limit the search with other facets, described below, and there are options for how we want the search to be conducted and the results formatted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8756e51-b24d-4f91-9e6b-5f69ceb74832",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"category\": \"newspaper\",    # The area of Trove we are searching. This parameter must be included, you can only search one area at a time. \n",
    "    \"q\": '\"empire loyalty\"',                    # Search query. See options described above. \n",
    "    # \"l-artType\": \"newspaper\", # filter by record type. newspaper options: 'newspaper', 'gazette'\n",
    "    # \"l-category\":             # filter by category. newspaper options: 'Article', 'Advertising', 'Details lists, results, guides', 'Family notices', 'Literature'\n",
    "    # \"l-title\": \"\",            # filter by the newspaper or gazette title id.\n",
    "    # \"l-decade\": \"\",           # filter by publication decade, formatted as YYY (e.g. 199 represents 1990 – 1999).\n",
    "    # \"l-year\": \"\",             # filter by publication year. For newspapers, only available if the decade facet is also applied. formatted as YYYY\n",
    "    # \"l-month\": \"\",            # filter by publication month. Only available if the decade and year facets are also applied. formatted as 1, 2, 3 etc\n",
    "    # \"l-illustrated\": \"\",      # filter by if the newspaper article is illustrated? options: true, false\n",
    "    # \"l-illustrationType\": \"\", # filter by type of illustration for newspaper article\n",
    "    # \"l-wordCount\": \"\",        # filter by newspaper article word count. options: \"<100 Words\", \"100 - 1000 Words\", \"1000+ Words\"\n",
    "    # \"l-state\": \"\",            # filter by state of publication for newspaper article\n",
    "    # \"facet\": \"all\",           # all the parameters beginning with \"l-\" are limiting by facet. The same parameters can be included as values in the results\n",
    "    # \"include\": \"\",            # newspaper options: tags, comments, lists, years (other options to get article text require NLA permission)\n",
    "    \"sortby\": \"dateasc\",        # options: datedesc, dateasc, relevance\n",
    "    \"bulkHarvest\": \"true\",      # Include this parameter if you intend to harvest a set of records for further processing in your own system. \n",
    "    \"encoding\": \"json\",         # options: xml, json\n",
    "    \"n\": 50,                    # The number of results to return per category. Maximum is 100. Default is 20.\n",
    "    \"reclevel\": \"full\",         # Indicates whether to return a full or brief metadata record. options: full, brief. \n",
    "    }                               # 'Brief' provides columns: 'id', 'url', 'heading', 'category', 'title_title', 'title_id', 'date', 'page', 'pageSequence', 'relevance_value', 'relevance_score', 'snippet', 'troveUrl'                               \n",
    "                                    # 'Full' adds the columns: 'identifier', 'trovePageUrl', 'illustrated', 'wordCount', 'correctionCount', 'tagCount', 'commentCount', 'listCount', 'lastCorrection_by', 'lastCorrection_lastupdated', 'pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d57696-b6d4-4c25-8db0-bcdaf18e703b",
   "metadata": {},
   "source": [
    "We can then take these variables, as well as our API_KEY, and use the function\n",
    "\n",
    "We have created the functions `get_records_count()` and `get_records_data()`, which use the requests library to conduct a search with the Trove API. Both take the arguments:\n",
    "- *url*: The base url.\n",
    "- *params*: The search parameters\n",
    "- *api_key*: The Trove API key\n",
    "\n",
    "`get_records_count()` prints out the number of records, and the search terms. \n",
    "`get_records_data()`returns the metadata results. It also has the optional argument:\n",
    "- *all_pages*: If you want to return all pages of the result. default is True.\n",
    "\n",
    "It's important to first see how large your dataset will be. So first, lets start with only a single page, so we can see how many results there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e313140-277f-4682-98b5-42e670685bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_records_count(url=url, params=params, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeadab66-9955-4541-9601-c2af5cce1ef1",
   "metadata": {},
   "source": [
    "Then we can actually run the search: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7dbd8-d15b-455f-9076-9dadd92e56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_records_data(url=url, params=params, api_key=API_KEY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c3883-a2c8-43af-bb32-2b2f578497f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if you have a very large number of records to retrieve, I recommend breaking the search up by year like this: \n",
    "\n",
    "# data = []\n",
    "# for year in range(1837, 2022):\n",
    "#     print(year)\n",
    "#     str_year = str(year)\n",
    "#     params[\"l-decade\"] = str_year[:3]\n",
    "#     params[\"l-year\"] = str_year\n",
    "#     try:\n",
    "#         records = get_records_data(url=url, params=params, api_key=API_KEY)\n",
    "#         data.extend(records)\n",
    "#     except TypeError:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62674224-0c91-4f8d-962e-c116d54ef8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a63a9-32b0-4454-9e00-71d799976d87",
   "metadata": {},
   "source": [
    "We can then turn this result into a 'dataframe', with each record a new row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291746c7-e415-4a29-bc85-0c03b88a9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1cdd7-6d90-432b-9629-575f7f82dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab44f5-ec53-4ab0-93c2-54bd9c73b1aa",
   "metadata": {},
   "source": [
    "which we can also save as a csv file for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e65ea-7a64-4c0b-a290-d4e86fe1cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_name = \"\"\n",
    "for key, val in params.items():\n",
    "    if key in ['category', 'q'] or key.startswith('l-'):\n",
    "        search_name += '_' + val.replace(' ', '_').replace('\"', '').rstrip('_')\n",
    "search_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e84c51-2ab8-442e-8214-97855ffb1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'../data/trove_search{search_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b01e9d-bd49-42ff-a98e-075a69a74296",
   "metadata": {},
   "source": [
    "#### Importing a csv file\n",
    "The Pandas package can read in a CSV file and turn it into a dataframe. If the file is in a different folder than your notebook, you will need to include the filepath (eg folder names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59e748-1b11-48a6-bd5e-0ce30f64ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/trove_search_white_australia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093fbfa-e436-4a58-a049-c9a18fa2e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4451c7-d52a-462f-8e86-63adc822a4ec",
   "metadata": {},
   "source": [
    "#### Cleaning the data\n",
    "Some columns contain dictionaries or lists, which we might want to split up. But when we read them in from a csv, they can be there as strings or text value. \n",
    "\n",
    "Here we go through each column, and check that it is an 'object' (as opposed to integer) and that it's not a dictionary already. if it meets those conditions, we search for if all the values that aren't N/A start with a { (dictionary) or [ (list), and if so, convert them. \n",
    "\n",
    "We then check if there are columns where all the strings are numbers, and convert them to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb75b85-a18d-4bc4-86ec-920a015c3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == object and not df[col].apply(lambda x: isinstance(x, dict)).sum():\n",
    "        if df[col].dropna().str[0].isin(['{', '[']).sum() + df[col].isna().sum() == df.shape[0]:\n",
    "            df[col] = df[col].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else x)\n",
    "        elif (df[col].dropna().str.isdigit().sum() + df[col].isna().sum()) == len(df):\n",
    "            print(f\"{col} should be an int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5ccb-d7ce-42fd-babb-e7b555b2b4cd",
   "metadata": {},
   "source": [
    "Now that we have done that, we can explode out the dictonaries, making new columns with the key value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd89725-d7c6-4975-a505-75a0efcd3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].apply(lambda x: isinstance(x, dict)).sum():\n",
    "        df = explode_dictonaries(df, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3141dff-fb34-4063-9fd2-42e9602727ba",
   "metadata": {},
   "source": [
    "Trove newspaper title columns have the name, location, and run of the title all in one column - we can split those with the function `split_trove_title()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480336c-5570-44d8-be02-c27d6fffa725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = split_trove_title(df, title_col='title_title').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f32d95-bf9d-4ba6-a01b-7bdcc79bb59a",
   "metadata": {},
   "source": [
    "Sometimes if a record is no longer available, we will still get a record id and links, but the rest of the columns are blanks. Below will check what percentage of a row is N/A, and if its above 80%, we drop the row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9bcab0-66b2-483c-ad68-0b3e1094790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['na_percentage'] = df.isna().sum(axis=1) / df.shape[1] * 100\n",
    "df = df.loc[df['na_percentage']<=80].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1f96c-02df-483d-a65d-63ed1f9a15ba",
   "metadata": {},
   "source": [
    "Now we want to add a year column, for our visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aec546-d6fd-4a2c-8793-27cfab2f1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(df.columns.get_loc('date')+1, \n",
    "          'year', \n",
    "          df['date'].str[:4].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae659b-ba09-44ef-906e-4f570544bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab0304-6900-4c08-a57d-88acb82bce0f",
   "metadata": {},
   "source": [
    "### Visualising the data!\n",
    "There are many different visualisation libraries available, here we are using [Plotly](https://plotly.com/python/) Express to show some of the kinds of visualisations you can make. \n",
    "\n",
    "\n",
    "#### Create a bar chart of a column\n",
    "To create a stacked bar chart, first we need to accumulate the counts. Fortunately, that's easy to do using the `groupby()` function, and applying the size method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebb2cd-dcaa-4280-b36a-56b9cd0335ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = 'White Australia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf7c76-bea3-4da0-bbb1-38ef0601d18f",
   "metadata": {},
   "source": [
    "But if we want accumulated counts, that needs to be done first. Fortunately, that's easy to do using the `groupby()` function, and applying the size method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a186ed-25de-4056-926d-3950ac95a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby('newspaper_state').size().reset_index(name='count')\n",
    "\n",
    "fig = px.bar(counts, x=\"newspaper_state\", y=\"count\", color='newspaper_state', title=f'{title_name} Results by State')\n",
    "fig.update_traces(hovertemplate='<b>%{x}</b><br>Total: %{y}<extra></extra>')\n",
    "\n",
    "fig.update_traces(\n",
    "    hovertemplate='<b>%{x}</b><br>Total: %{y}<extra></extra>'\n",
    ")\n",
    "\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad32b5-a16a-4c19-af99-53739ee69f22",
   "metadata": {},
   "source": [
    "Another possible bar chart would be looking at the results per newspaper. For this, it is clearer to read when it's a horizontal bar chart, and we may need to adjust the height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5f240-10b6-430a-920d-7fba639e4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby(['newspaper_title', 'newspaper_state']).size().reset_index(name='count')\n",
    "\n",
    "fig = px.bar(counts, x='count', y=\"newspaper_title\", color='newspaper_state', orientation='h', height=500, title=f'{title_name} coverage in newspapers')\n",
    "\n",
    "fig.show('notebook')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b0f0f-ce00-43bd-871e-acbefb8aa30b",
   "metadata": {},
   "source": [
    "Earlier we looked at how many articles per year there were, but what about the word count? Does this change things?\n",
    "\n",
    "Names of different colour scales are listed at [https://plotly.com/python/builtin-colorscales/](https://plotly.com/python/builtin-colorscales/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52ac77-7e21-45e9-9946-de220ad86b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby('year')['wordCount'].sum().reset_index()\n",
    "fig = px.bar(counts, x='year', y=\"wordCount\", color=\"wordCount\", color_continuous_scale=px.colors.sequential.Agsunset_r, title=f'{title_name} wordcount by year')\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611cde4-f838-4722-ba37-946039f10915",
   "metadata": {},
   "source": [
    "What about a scatterplot? This time we want individual dots for each, so we don't need to aggregate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f94508-f9b6-4529-bc36-a885428288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"year\", y=\"newspaper_state\", color=\"relevance_value\", size='wordCount', title=f'{title_name} coverage per state per year, noting relevance and word count')\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5313500-5743-4435-bece-ff740217a0c9",
   "metadata": {},
   "source": [
    "We can also create word clouds with the Python package WordCloud and the headline column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cd934-1574-4d11-9b2a-e0ccb5ae26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(str(i) for i in df['heading'])\n",
    "title=f'Word Cloud of Headlines Relating to {title_name}'\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
    "\n",
    "fig = px.imshow(wordcloud)\n",
    "fig.update_layout(\n",
    "    title=title,\n",
    "    xaxis_visible=False,\n",
    "    yaxis_visible=False, \n",
    ")\n",
    "fig.show('notebook')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33661e27-6936-4651-beff-c248fb8d4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.update([\"Mr\", \"S\"])\n",
    "wordcloud = WordCloud(stopwords=stopwords, max_words=300, width=1000, height=500, background_color=\"black\").generate(text)\n",
    "\n",
    "fig = px.imshow(wordcloud)\n",
    "fig.update_layout(\n",
    "    title=title,\n",
    "    xaxis_visible=False,\n",
    "    yaxis_visible=False,\n",
    ")\n",
    "fig.show('notebook')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09811c81-0183-4640-8155-ab67481738a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e96f21-3e74-4ded-a598-0b28377ee6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
